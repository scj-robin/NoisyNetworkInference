%!TEX root = VEMScoreEdges.tex



\subsection{Review of the literature}
Notes on identifiability based on papers : 

\begin{itemize}
\item \cite{AMR09} : "Allman, Elizabeth S. and Matias, Catherine and Rhodes, John A." : \emph{Identifiability of parameters in latent structure models with many observed variables}
\item \cite{AMR10} "Allman, Elizabeth S. and Matias, Catherine and Rhodes, John A." :  \emph{Parameter identifiability in a class of random graph mixture models}
\item \cite{T63}"Teicher, Henry":    \emph{Identifiability of Finite Mixtures}
\item \cite{T67} "Teicher, Henry":    \emph{Identifiability of Mixtures of product measures}
\end{itemize}

\vspace{2em}


\paragraph{What is done in    \cite{AMR10}}: identifiability in weighted SBM 
\begin{eqnarray*}
S_{ij} | Z_i =k ,Z_j = \ell &\sim& \mu_{k\ell}\\
  \mu_{k\ell} &=& (1-\gamma_{k\ell})  \delta_ {\{0\}} + + \gamma_{kl} F_{k\ell}( \cdot)
\end{eqnarray*}
 for \underline{uni dimensional $S$ and symmetric}  with 
\begin{itemize} 
\item \underline{ $ F_{k\ell}( \cdot)$ parametric} (Theorem 12 of \cite{AMR10})  : $ F( \cdot; \theta_{k\ell})$ under the following assumptions: 
\begin{enumerate}
\item[[A1]]  The $K(K+1)/2$ parameter values $\theta_{k\ell}$ are distinct
\item[[A2]]  The family of measures $\Mcal = \{ F( \cdot; \theta) | \theta\in\Theta\}$  is such that 
\begin{itemize}
\item[[A2](i)] all elements of $\Mcal$ have no point mass at $0$ 
\item[[A2](ii)] the parameters of finite mixtures of  measures of $\Mcal$ are identifiable (up to label switching) i.e.
$$ \sum_{m=1}^M \alpha_m F(\cdots, \theta_m) =  \sum_{m=1}^M \alpha'_m F(\cdots, \theta'_m) \Rightarrow  \sum_{m=1}^M \alpha_m \delta_{\theta_m} =  \sum_{m=1}^M \alpha'_m \delta_{\theta'_m} $$ 
\end{itemize}

In particular : true for Gaussian (\cite{T63}) and Laplace. 
\end{enumerate}
\item \underline{ $ F_{k\ell}( \cdot)$ non-parametric} (Theorem 14 of \cite{AMR10}) : if the $\mu_{k \ell}$ are \emph{linearly independent} (to be detailed)
\end{itemize}


\paragraph{About the demonstrations} 

\begin{itemize}
\item  \emph{Parametric case}  It is done from the distribution of a triplet ($S_{ij},S_{ik}, S_{jk}$) and using \cite{T63}. How to adapt it  to our case? %because only relies on \cite{T63} and then it is really simple. 
\item  \emph{Nonparametric case} : only depends on the linear independancy of the $\mu_{k\ell}$. We have to precise it for our case? 
\end{itemize}


\subsection{Proof in the parametric uni-dimensional context}

I tried to mimic/extend the proof of \cite{AMR10}  but I don't think we are in the same scope.  

\paragraph{Distribution of the $S_{ij}$}

 \begin{align*}
 \mathbb{P}(S_{ij}) &= \sum_{q,\ell} \pi_{q} \pi_{\ell} [ (1-\gamma_{q\ell})F_0(S_{ij}) + \gamma_{q\ell} F_1(S_{ij} )]\\
&= \left[1-  \sum_{q, \ell}  \pi_\ell \pi_q  \gamma_{q,\ell} \right] F_{0}(S_{ij}) +   \left[ \sum_{q, \ell}  \pi_q \pi_\ell    \gamma_{q,\ell} \right] F_{1}(S_{ij})\\
 \end{align*}
So \textbf{assuming that $F_0$ and $F_1$ are such that any mixture of those two distributions is identifiable}, we obtain the identifiability of $\theta_0$, $\theta_1$ and $ \sum_{q, \ell}  \pi_{\ell} \pi_q   \gamma_{q,\ell}$. 


So we have identifiability of $\pi'  \gamma  \pi$. 
It seems to me that once we have identified $\theta_0$ and $\theta_1$ we will be able to apply to proof of CÃ©lisse \& al. \cite{CDP12}, which is the one I know better. Which is the thing you said : meaning that once we have identified to high level, we are identifiable just like any binary SBM. 







\paragraph{Distribution of the triplet  ($S_{ij},S_{ik}, S_{jk}$)}

\begin{align*}
\mathbb{P}(S_{ij},S_{ik}, S_{jk}) &= \sum_{q,\ell,m} \pi_{q} \pi_{\ell} \pi_{m} [ (1-\gamma_{q\ell})F_0(S_{ij}) + \gamma_{q\ell} F_1(S_{ij} )] [(1-\gamma_{qm})F_0(S_{ik}) + \gamma_{qm} F_1(S_{ik})]\\
&   [ (1-\gamma_{\ell m})F_0(S_{jk}) + \gamma_{\ell m} F_1(S_{jk})]\\
&= \sum_{q, \ell,m} \sum_{(u,v,w) \in \{0,1\}^3}  \eta_{q,\ell,m,u,v} F_{u}(S_{ij}) F_{v}(S_{ik}) F_{w}(S_{jk})\\
&= \sum_{(u,v,w) \in \{0,1\}^3}  \left(\sum_{q, \ell,m} \eta_{q,\ell,m,u,v}\right) F_{u}(S_{ij}) F_{v}(S_{ik}) F_{w}(S_{jk})\\
&=  \sum_{(u,v,w) \in \{0,1\}^3}  \left(\sum_{q, \ell,m} \eta_{q,\ell,m,u,v}\right) F_{u,v,w}(S_{ij},S_{ik},S_{jk})
\end{align*}
with $$\eta_{q,\ell,m,u,v} = \pi_{q} \pi_{\ell} \pi_{m} (1 - \gamma_{q\ell})^{1-u}   \gamma_{q\ell} ^{u}  (1 - \gamma_{q\ell})^{1-u}   \gamma_{q\ell} ^{u}  (1 - \gamma_{qm})^{1-v}   \gamma_{qm} ^{v} (1 - \gamma_{\ell m })^{1-w}   \gamma_{\ell m } ^{w}.$$  \\
The distribution of $(S_{ij},S_{ik}, S_{jk})$ is a mixture  (weights  = $ \sum_{q, \ell,m} \eta_{q,\ell,m,u,v}$) of   the following distributions 
$$ F (s) =  F_u(s_1,\theta_u) F_v(s_1,\theta_v)  F_w(s_1,\theta_w) $$ 
where $F \in \Fcal$   with 
$$ \Fcal  = \{ F (s; \theta_0, \theta_1)  :  F (s; \theta_0, \theta_1)  = F_u(s_1,\theta_u),F_v(s_2,\theta_v) F_w(s_3,\theta_w) , (u,v,w) \in \{0,1\}^3,  \theta_0, \in \Theta_0, \theta_1 \in \Theta_1\}$$


\noindent \textbf{Asumptions};  
\noindent  [A1]  we assume that any mixtures of elements of $\Fcal$ is identifiable.  (to develop to get assumptions on $F_0$ and $F_1$). 



The, under assumption [A1], we have : 



Then using Theorem 1 of \cite{T67} we have the identifiability of any mixture of the  product  measures. 




%------------------------------------------------------------------------------
\subsection{Notes from the 20/11/19}
%------------------------------------------------------------------------------

%------------------------------------------------------------------------------
\paragraph{Preliminary remarks.}
Let $nSBM(\pi, \gamma, F_0, F_1)$ denote the noisy SBM model and $SBM(\pi, \alpha)$ the standard binary SBM. \cite{CDP12} showed that $SBM(\pi, \alpha)$ is identifiable provided that all $\overline{\alpha}_k = \sum_\ell \pi_\ell \alpha_{k\ell}$ are different.

\begin{lemma} \label{lem:SBMthres}
 Let $S \sim nSBM(\pi, \gamma, F_0, F_1)$ and define $B_{ij} = \Ibb\{S_{ij} \leq t\}$. We have that 
 $$
 B(t) := [B_{ij}(t)] \sim SBM(\pi, \alpha(t))
 $$
 where, denoting $\Delta (t) = F_1(t) - F_0(t)$,
 $$
 \alpha_{k\ell}(t) 
 = \gamma_{k\ell} F_1(t) + (1 - \gamma_{k\ell}) F_0(t)
 = F_0(t) + \gamma_{k\ell} \Delta  (t). 
 $$
\end{lemma}

\begin{lemma} \label{lem:identifSBMthres}
  If the model $SBM(\pi, \gamma)$ is identifiable then the model $SBM(\pi, \alpha(t))$ is identifiable as soon as $\Delta F(t) \neq 0$.
\end{lemma}

The proof follows: the identifiability of $SBM(\pi, \gamma)$ means that all $\gammabar_k  = \sum_\ell \pi_\ell \gamma_{k\ell}$ are different, so because all 
$$
\overline{\alpha}_k(t) = \sum_\ell \pi_\ell \alpha_{k\ell}(t) = F_0(t) + \gammabar_k \Delta F(t)
$$
are different as soon as $\Delta F(t) \neq 0$.

%------------------------------------------------------------------------------
\paragraph{Parametric case}
Suppose that $F_0$ and $F_1$ belong to a same parametric family, the mixture of which are identifiable. If $S \sim nSBM(\pi, \gamma, F_0, F_1)$, then the marginal distribution is the mixture
$$
S_{ij} \sim \gammabbar F_1 + (1 - \gammabbar) F_0,
$$
which is identifiable so $\gammabbar$, $F_0$ and $F_1$ are identifiable.

Assuming that $SBM(\pi, \gamma)$ is identifiable, we use Lemma \ref{lem:identifSBMthres}, picking a threshold $t$ such that $F_0(t) \neq F_1(t)$, to prove the identifiability of $\pi$ and $\alpha(t)$. \textcolor{red}{A VERIFIER:} $\gamma$ can then be retrieved by solving the $K(K+1)/2$ equations relating each $\gamma_{k\ell}$ with each $\alpha_{k\ell}(t)$. 

%------------------------------------------------------------------------------
\paragraph{Non-parametric case}
Our aim is to show that if $nSBM(\pi, \gamma, F_0, F_1)$ and $nSBM(\pi', \gamma', F'_0, F'_1)$ yields the same distribution, then necessarily, $\pi = \pi'$, $\gamma = \gamma'$, $F_0 = F'_0$, $F_1 = F'_1$. We assume that $F_1 \succ F_0$, so that $\Delta(t) \neq 0$ for all $t$. If we further assume that $SBM(\pi, \gamma)$ is identifiable, Lemma \ref{lem:identifSBMthres} ensures the identifiability of $SBM(\pi, \alpha(t))$ for all $t$.

So assume that,  any $t \in \mathbb{R}$,  $\alpha(t) = \alpha'(t)$, then, 
$$
\begin{array}{crcl}
& (1-\gamma_{k\ell}) F_0(t) + \gamma_{k\ell} F_1(t) &=& (1-\gamma'_{k\ell}) F'_0(t) + \gamma'_{k\ell} F'_1(t) \\
\Leftrightarrow&
 F_0(t) +\gamma_{k\ell}  \Delta(t) &=&F'_0(t) +  \gamma'_{k\ell} \Delta'(t)\\
 \mbox{ where }& \Delta(t) &=&F_1(t)-F_0(t)
\end{array}
$$
This equality is true for any $(k,\ell,t)$. 

As a consequence
\begin{eqnarray*}
 \gamma_{k\ell} &=& \frac{F'_0(t)  - F_0(t)}{\Delta(t)}+  \gamma'_{k\ell} \frac{\Delta'(t)}{\Delta(t)}\\
 &=& A(t) + \gamma'_{k\ell}B(t), \quad \forall t  \in \mathbb{R}
 \end{eqnarray*}
 (we used the fact that $\forall t \in \mathbb{R}$, $\Delta (t) = F_1(t) - F_0(t) \neq 0$. )
 
 Let us consider two pairs $(k,\ell)$ and $(k',\ell')$, we have 
\begin{eqnarray*}
 \gamma_{k\ell} &=& A(t) + \gamma'_{k\ell}B(t)\\
 \gamma_{k'\ell'} &=& A(t) + \gamma'_{k'\ell'}B(t)
 \end{eqnarray*}
So, if \textcolor{red}{$ \gamma'_{k\ell} \neq  \gamma'_{k'\ell'}$}
$$B(t)  = \frac{ \gamma_{k\ell}  -  \gamma_{k'\ell'} }{  \gamma'_{k\ell}-\gamma'_{k'\ell'} }
$$ 
 
 So $B(t)$ is a constant function: 
 $B(t) = \frac{\Delta'(t)}{\Delta(t)} = \frac{F'_1(t) - F'_0(t)}{F_1(t) - F_0(t)}  = B$
 and $B>0$. 
 
 Moreover we get $$  \gamma_{k\ell}  -  \gamma_{k'\ell'}  = B ( \gamma'_{k\ell}  -  \gamma'_{k'\ell'}),\quad  \forall (k,\ell,k',\ell').$$ 
  
 
Hence 
 $$ F_1(t) - F_0(t) = B (F'_1(t) - F'_0(t))$$
 So 
 $$F_0(t) = F_1(t) - B  F'_1(t) +B  F'_0(t)$$
 
 Since $t  \mapsto B(t)$ is constant then  $t  \mapsto A(t)$ is also a constant. 
So 
$$ A(t) = \frac{F_0'(t)-F_0(t)}{\Delta(t)} = A$$
so
\begin{eqnarray*}
 F'_0(t) &=& F_0(t) +  A(F_1(t) - F_0(t))\\
 F'_0(t) &=& (1-A) F_0(t) + A F_1(t)\\
F_0(t) &=& \frac{1}{1-A} F'_0(t) - \frac{A}{1-A} F_1(t)\\
F_0(t) &=& B F'_0(t) + F_1(t)  - B F'_1(t) 
\end{eqnarray*}
As a consequence, 
\begin{eqnarray*}
 \frac{1}{1-A} F'_0(t) - \frac{A}{1-A} F_1(t) - B F'_0(t) - F_1(t)  +B F'_1(t) &=& 0\\
\frac{1}{1-A} F'_0(t) - \left(1+ \frac{A}{1-A}\right) F_1(t) - B F'_0(t) +B F'_1(t) &=& 0\\
 \frac{1}{1-A} F'_0(t) - \frac{1}{1-A} F_1(t) - B F'_0(t) +B F'_1(t) &=& 0\\
  \end{eqnarray*}
  So
  \begin{eqnarray*}
 F_1(t) &=&(1- B(1-A)) F'_0(t) + B(1-A)F'_1(t)\\
 F_0(t) &= &B F'_0(t) + (1- B(1-A)) F'_0(t) +B(1-A)F'_1(t) - BF'_1(t)\\
 &=& (1- B(1-A)+B)F'_0(t) -AB F'_1(t)\\
 &=& (1 + AB)F'_0(t) -  AB F'_1(t)\\
 &=& F'_0(t)  - AB( F'_1(t) -  F'_0(t)) \\
 F_0(t) - F'_0(t)&=&  AB( F'_0(t) -  F'_1(t)) 
  \end{eqnarray*}

 


\textcolor{red}{A FINIR} : il va falloir jouer sur le support des $F_1$ et $F_0$. 


%\begin{itemize}
%\item Par exemple si on suppose que  $F_0(t) = F'_0(t) = 0 $ pour un certain $t$ (donc support de $F_0$ bornÃ© infÃ©rieurement) 
%Alors $AB F'_1(t) = 0$ pour ces $t$ mais $F_1(t) > F_0(t) =0 $ donc $A=0$ ou $B=0$. Mais $B>0$ donc $A = 0$ donc $F'_0(t) = F_0(t)$. Donc $F_1(t) = F'_1(t)$ et on a fini.... 
%
%Mais c'est fort comme hypothÃ¨se. Et surtout Ã§a me paraÃ®t contre intuitif???? \\
%\textcolor{red}{En fait : }

On a $F_1(t) < F_0(t)$ si on veut que en moyenne les valeurs sous $F_1$ soient plus grandes que celles sous $F_0$. 


 Si il existe  $\tau$ tel que $F_0(\tau) = 1 = F_0'(\tau)$ alors en ce point, 
  $ 1 = 1 - AB (F'_1(\tau) - F'_0(\tau))$ donc  $ AB (F'_1(\tau)-1)$; 
  Or  $F'_1(\tau) < F'_0(\tau)$ donc $AB = 0$. Or $B>0$ donc $A = 0$ Donc $F_0'(t) = F_0(t)$. \\
  
  
  

   
On peut peut-Ãªtre seulement mettre des vitesses sur les queues de distributions et travailler en limite? 







